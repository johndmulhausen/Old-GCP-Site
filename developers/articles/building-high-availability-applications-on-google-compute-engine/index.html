{% comment %}
This document is sourced from GitHub. 
Do not change this file in a CL!
Instead, edit me at: 
https://github.com/GoogleCloudPlatformSite/GoogleCloudPlatformSite.github.io/edit/master/developers/articles/building-high-availability-applications-on-google-compute-engine.html
{% endcomment %}
<!DOCTYPE html>
<html devsite>
<head>
    <title>Building High Availability Applications on Google Compute Engine </title>

    
      <meta name="description" content="Google Cloud Platform lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.">
    

    <meta name="hide_page_heading" value="true" />
    <meta name="full_width" value="true" />
    <meta name="viewport" content="initial-scale=1, minimum-scale=1, width=device-width">
    <meta name="google-site-verification" content="8dOEM3Xenm6yaBc83y2WKgqQG0iHI7Ph6Rl_YLIZLQ8" />
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400italic,300,400,600,700" rel="stylesheet">
    <link href="/css/default.css" rel="stylesheet">

    <!--[if lt IE 9]>
    <link rel="stylesheet" media="screen" href='/c/cp-ie.css'>
    <![endif]-->

    <!-- GSAP animation frameworks -->
    <script src="//www.gstatic.com/external_hosted/gsap/TweenMax.min.js"></script>
    <script src="//www.gstatic.com/external_hosted/gsap/TimelineMax.min.js"></script>
    <script src="/js/floodlight.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
</head>
<body>

<div id="maia-main" class="cp-article">
  <div class="maia-cols">
    <div class="maia-col-9">

      <div>
        <div style="float:right"><div class="g-plusone"></div></div>
        <h1 class="title">Building High Availability Applications on Google Compute Engine</h1>
      </div>

      
<div class="cp-article-tutorial">

  <h2>Introduction</h2>
  <p>Google Compute Engine is a platform for developers who want to implement large, distributed systems using virtual machines. To build high availability applications, developers must design their systems to handle both planned and unplanned failures of components from a single instance all the way up to an availability zone. This paper provides guidance for building high availability applications on Google Cloud Platform.</p>

  <p class="special">
    The content in this paper and related sample application are deprecated and do not reflect the latest Google Cloud Platform technology. Please refer to <a href="/">https://cloud.google.com/</a> for the latest papers, technical articles, and sample applications.
  </p>

  <p>Access to Google&rsquo;s cloud technologies now makes it possible for developers to build globally distributed applications by leveraging Google&rsquo;s global footprint. Google performs upgrades and routine maintenance to maintain a consistent, high-performance infrastructure.  By building robust applications, developers can be insulated from an unavailable zone whether it is caused by an unexpected outage or planned maintenance window. Managed Google services such as Cloud SQL, Cloud Datastore, and Cloud Storage can be utilized to reduce the impact of maintenance windows or unplanned zone outages, since these services are not tied to a specific zone. By following the best practices for building robust systems across multiple zones, the impact of maintenance windows is significantly reduced.</p>

  <p>This paper serves as an introductory guide for building distributed, high availability applications on Compute Engine. An example application is presented and the processes and best practices for migrating instances around maintenance windows are discussed.</p>

  <h2>Example Application</h2>
  <p>Although there are almost countless ways to build distributed systems on Compute Engine, this paper focuses on a common web application example. The general pattern, in which most of the servers are stateless and the persistent data is replicated, is common among many types of distributed systems. Additionally, load balancing is used to distribute the requests among a pool of stateless servers. The following figure highlights the core components of the web application architecture and one potential multizone implementation.</p>

  <figure>
    <img src="/images/articles/building-high-availability-applications-on-gce/web-overview.jpg">
    <figcaption>Figure 1: Web application example overview</figcaption>
  </figure>

  <p>It is important to note that this is just one of the common implementation patterns and may not be suitable for other web applications. For example, the database servers in the solution are configured in a master-slave setup. If a NoSQL database is used, it is often possible to eliminate synchronous cross-zone calls.</p>

  <p>A managed Domain Name System (DNS) service resolves all requests for the web application and distributes the load to multiple load balancer servers. As of September 2013, Google does not provide a managed DNS service and external managed DNS providers should be used. The user traffic is handled by the software load balancers on Compute Engine instances. The load balancers are responsible for distributing the requests to web servers that contain the core application logic.</p>

  <p>Since applications should have a minimal amount of state stored in the frontend web servers, the core state must be maintained in database servers. The configuration in Figure 1 is frequently used for MySQL-based implementations that utilizes master-slave configuration for data redundancy. All web servers in this example are communicating to the master database located in zone B of region 1. It is easy to connect web servers in one region to a database in another region by utilizing the global private network that spans all Compute Engine regions.  However, cross-region network communication results in increased regional network charges and higher latency. As a result, it is important to evaluate the feasibility of deploying a multi-region implementation to understand the performance and cost implications.</p>

  <p>The final components of the web application architecture are the database and maintaining state in multiple zones to reduce potential data loss. In this example with a master database, the slave zones are responsible for syncing with the master database and maintaining an up-to-date copy of the database. It is highly recommended to significantly research and test MySQL before deploying a master-slave implementation in order to understand the potential modes of failure.</p>

  <p>The example architecture highlights a common implementation pattern, but it is technically possible to create connections between all components. There are many ways to expand this solution by including additional technologies, such as a distributed messaging system, distributed in-memory key-value servers, and distributed file system, but they are outside of the scope of this paper. However, many of the technologies used to migrate components between zones can also be applied to migration of these additional components. Additionally, Google Cloud Platform managed services, such as Cloud SQL, Cloud Datastore, and Cloud Storage, can be used to eliminate the number of components that must be migrated between zones.</p>

  <h2>Component Configuration and Migration</h2>
  <p>A best practice when building distributed systems on a cloud platform is to build each core component as a separate, repeatable implementation. This section focuses on how the core components of the example web application can be implemented with repeatable deployments and migrated around maintenance windows. By leveraging important features of Compute Engine, such as custom images, metadata servers, and startup scripts, it is possible to greatly simplify the manual intervention required to migrate instances between zones. Additionally, persistent disks and other core technology are critical for preventing data loss and building scalable implementations.</p>

  <h3>Load Balancer Configuration and Migration</h3>
  <p>The load balancer can be viewed as a high-throughput component with no business logic.  Although this section details configuring and migrating a load balancer, most steps are common to migrating any system between instances. The load balancer instance is responsible for distributing the requests sent to its IP address. Figure 2 highlights the key implementation components and the process for migrating between zones.</p>

  <figure>
    <img src="/images/articles/building-high-availability-applications-on-gce/load-balancer-config.jpg">
    <figcaption>Figure 2: Load balancer configuration and migration</figcaption>
  </figure>


  <p>To complete migration, the load balancer instance in the new zone must be created, configured, and registered with the DNS service. Most steps for migrating the load balancer actually focus on the configuration of the load balancer instance, to simplify the process of creating new instances.</p>

  <ol>
    <li><h4>Create new instance</h4>
      Create the instance by using the RESTful API, <span class="code">gcutil</span> command line tool, or the web console. An instance can be created directly from a custom image based on a preconfigured version of a base OS provided by Google.</li>
    <li><h4>Execute startup script</h4>
      After the instance is created, startup scripts and metadata allow automatic configuration of an instance beyond the configuration provided in the instance&rsquo;s image. Common startup configuration activities include updating packages, installing new packages, and utilizing metadata for instance-specific configuration. For example, metadata can be used to indicate that the instance is a load balancer and the specific software for the predefined function can be installed. If service account Google Cloud Storage access was configured during instance creation, <span class="code">gcutil</span> is automatically authenticated to access files stored in the project&rsquo;s storage buckets.</li>
    <li><h4>Install Load Balancing software</h4>
      Once the instance is configured, the load balancer software must be installed. The software can be downloaded from a standard software repository or a private cloud storage bucket. If the custom image used to create the instance already contains the load balancer software, only an update to a trusted version is recommended.</li>
    <li><h4>Register web servers with load balancers</h4>
      The next step in configuring the instance is to register the required web servers with the load balancer.  The internal DNS implementation on Compute Engine simplifies the process of finding other IP addresses since the instance name can be used directly. There are many different approaches to registering web servers with the load balancer. One option is to utilize a full list of web server IP addresses in the load balancer configuration and automatically send traffic after health checks. Otherwise, scripting can be used to add the new nodes to the load balancer configuration and complete a hot restart to maintain existing connections. It is possible to send requests to web servers in a different zone, and this design choice requires a full evaluation of latency and cost impacts. If the web servers are not available in the new zone, traffic can be sent temporarily to the existing region if the load balancer must be tested.</li>
    <li><h4>Add new load balancers to Managed DNS</h4>
      Before adding the load balancer to the Managed DNS, it is important to run a suite of automated integration tests over the load balancer to verify that everything is functioning correctly. After performance and functionality are verified, the IP address of the load balancer can be added to the Managed DNS to start serving live traffic. Static IP address are also available for Compute Engine instances and are recommended for load balancers. Since static IP address are bound to a region, developers can retarget IP addresses when migrating across zones within the same region.</li>
    <li><h4>Remove old load balancers from Managed DNS</h4>
      The following migration process can be repeated or automated to create the required number of load balancers in the new zone. After all load balancers are created and registered with the Managed DNS, the load balancers in the old zone must be deregistered from the Managed DNS. Once all of the load balancers in the old zone are no longer serving traffic, they should be terminated to reduce the cost of unused instances.</li>
  </ol>

  <p>Although it is reasonably straightforward to migrate a set of load balancers between zones, many of the configuration techniques and steps are relevant for any systems that require a repeatable deployment process.</p>

  <p>Web Server Configuration and Migration</p>
  <p>The web server component is generally stateless and contains the core business logic for handling requests from load balancers and other web servers. To satisfy incoming requests, the web servers typically connect to a database that maintains the stateful system data. To migrate web servers to a new zone, the web servers must be created and registered with the required components to start successfully serving traffic. The following figure presents the overall architecture and highlights the core web server components used in the example application:</p>

  <figure>
    <img src="/images/articles/building-high-availability-applications-on-gce/web-server-config.jpg">
    <figcaption>Figure 3: Web server configuration and migration</figcaption>
  </figure>

  <p>The process of moving a web server should be automated as much as possible because it is not feasible to manually move the large number of web servers in a typical deployment. The key steps for migrating a single web server are described in the following section:</p>

  <p>Create new instance<br>
    The new instance must be created and configured to build out the web server software stack. This process is almost identical to configuring the load balancer instance, except that different instance types and tags should be used for optimal performance and security.  The instance creation can be automated either by using deployment tools or by building a simple deployment layer on top of the RESTful API.</p>

  <ol>
    <li><h4>Execute startup script</h4>
      After the instance is created and running, the startup script is automatically executed. It can utilize the metadata to create a unique instance configuration. If the web application server requires a scratch disk larger than the root disk, the startup script can format and mount the disk requested during instance creation. Ideally, the following steps are handled automatically by the startup script, but occasionally manual intervention may be necessary.</li>
    <li><h4>Format and mount persistent disk</h4>
      A persistent disk is not always required for web servers, but it can be required when a web server contains critical data that cannot be lost if the instance terminates unexpectedly. It is also possible to mount a persistent disk as read-only to multiple instances if files must be obtained by multiple instances from a block storage device. If a persistent disk must be attached to a web server, the startup script can mount the required disk.</li>
    <li><h4>Install required packages</h4>
      Once the required disks are attached, the next step is to install the required default web application stack, such as Apache.  The default web application can be installed from a software repository, cloud storage, custom image, or a read-only persistent disk.</li>
    <li><h4>Instance custom software</h4>
      The web server must be customized with the required software to execute the required business logic within the example application. The exact configuration can be deployed from a startup script utilizing metadata to identify the builds and packages that must be installed from cloud storage or other sources.</li>
    <li><h4>Connect web server to database servers</h4>
      After the web server is fully configured, a connection with the database server must be established. Typically setting up the database connection involves updating a configuration file or specifying the database hosts when the web servers initialize. If the database servers are already running in the new zone, a connection can be directly made to the same zone to reduce latency and cost. Otherwise, for high availability or rolling migration, the web server can connect to the database servers in the other zone. The web servers can directly connect to the database hosts by using the instance name as a DNS name. Otherwise, a separate service or metadata can be utilized to configure the connections.</li>
    <li><h4>Register web servers with load balancer</h4>
      The final step of the web server migration is to register with the load balancers to start serving live production traffic.</li>
  </ol>

  <p>For a fully automated migration, the startup script is the key component for orchestrating the configuration, testing, and registration details.  The process can be simplified further by utilizing managed Google Cloud Platform services, such as Cloud SQL and Cloud Storage. For example, if the database servers leverage MySQL, Cloud SQL can replace the MySQL servers and significantly reduce the challenges associated with migrating MySQL between zones. Additionally, if the web application utilizes a distributed file system for storing and serving files, Cloud Storage is a scalable object store service that can seamlessly scale to store and serve all required files. This process for migrating a web server provides just one example of a possible implementation.</p>

  <h3>Database Configuration and Migration</h3>
  <p>Database servers are among the most complex components to migrate between zones. They are critical because they maintain the majority of an application's state while serving a large number of requests. Although newer distributed database technology has improved the process of moving between zones, significant planning, automation, and testing is required to successfully create a new collection of database servers. The following figure gives an overview of the components involved in migrating a database server between zones.</p>

  <figure>
    <img src="/images/articles/building-high-availability-applications-on-gce/database-config.jpg">
    <figcaption>Figure 4: Database configuration and migration</figcaption>
  </figure>

  <p>The migration of a database server to a new zone shares many common processes with the migration of a web server and load balancer. The largest difference is maintaining the stateful, mission-critical data across persistent disks in multiple zones. The following steps detail the main processes involved in moving a database server to a new zone:</p>

  <ol>
    <li><h4>Enter maintenance mode (optional)</h4>
      Depending on the database technology utilized, the application&rsquo;s functionality may have to be reduced during the migration of critical data. Thorough knowledge of the database technology is required to understand the nuances of migrating servers between zones. The process can vary significantly, depending on the implementation. For example, MySQL Master/Slave configuration requires writes to be temporarily disabled until the slave in the new region is promoted to master. Conversely, NoSQL databases, such as Cassandra, greatly simplify creating new instances and replicating the data to a point where databases nodes can be migrated without going offline. If at any point the database needs to function in a reduced capacity, such as disabling writes, the web application must be designed to handle database write failures or provide a user-facing notification during the migration period.</li>
    <li><h4>Create new instance</h4>
      After the implications for migrating the data between database servers are understood, the next step is to create the required database instance.  Depending on the database requirements, the instance size can be configured to match the hardware requirements of the database technology. Configuring the network and tags for the database instances to accept only traffic from internal web servers and monitoring tools to increase the security is recommended.</li>
    <li><h4>Execute startup script</h4>
      The startup scripts and metadata are key components for automating the configuration and data replication of the database instance. Once the instance is running, the automatically executing startup script can use metadata to customize the instance. Additionally, the startup script should ideally set up and orchestrate the remaining steps of the migration process.</li>
    <li><h4>Format and mount persistent disk</h4>
      Storing all database files on persistent disk to prevent data loss if the instance unexpectedly terminates is highly recommended. The consistent performance of persistent disk lets developers design for a specific throughput. Databases on persistent disk may have higher performance than on a scratch disk in some cases because the number of random writes can be higher on persistent disk. Regardless of the choice of disks technology, the disks must be formatted and attached to each instance during the instance configuration.</li>
    <li><h4>Configure database backup process</h4>
      Once the required disks are attached to the instance and the database software is installed, the next step is to configure the database backup processes. Whether incremental or snapshot backups are captured from the database, Google Cloud Storage is the recommended location to store the resulting files.  Pre-configuring Google Cloud Storage access during instance creation allows for seamless utilization of the preauthenticated gsutil command line tool to upload backup files to a secure location .  It is also important to have a process in place to restore a database from backup files. Otherwise, the files are useless in a situation where data loss occurs.</li>
    <li><h4>Replicate data to new database</h4>
      The most complex step of migrating the database is moving and replicating the necessary database files. If the database tables can be restored completely by the files on disk, it is possible to complete a persistent disk snapshot and create a new persistent disk from the existing snapshot in any region. Any new writes after the snapshot has started are not guaranteed to be captured. As a result, writes must be disabled during the migration process.  Alternatively, if MySQL is utilized, the new zone can be configured as a slave to replicate the database tables. Writes must be disabled during the slave promotion process, but the time required is typically lower than the time to capture and restore a persistent disk snapshot. Finally, many NoSQL databases support adding new instances, replicating the data, and removing old instances without temporarily disabling writes. The process of migrating database files is a very complex topic. Thoroughly reviewing the selected database technology to understand the best practices for adding new instances is highly recommended.</li>
    <li><h4>Connect web server to database servers</h4>
      The final step of migrating a database instance is updating any required information in the web servers for specifying database location and access. Once the web servers are connected to the new database, the web application can exit maintenance mode if maintenance mode was required for data consistency.</li>
  </ol>

  <p>There are many database technologies and instance migration configurations, but the core components of utilizing persistent disk, startup scripts, metadata, and Google Cloud Storage are relevant for all implementations. After the database servers are migrated, and the entire application stack is running in the new zone, the previous instances can be safely terminated to reduce unnecessary costs.</p>

  <h2>Future Considerations</h2>
  <p>As Google Cloud Platform continues to add functionality and launch innovative new products, it will become easier than ever to deploy scalable cloud applications. For example, the challenges associated with architecting around maintenance windows will be reduced through new features and functionality. Features such as load balancing service, global persistent disk snapshots, persistent root disks are examples of how Google continues to provide important Compute Engine functionality. The load balancing service offers server-side load balancing and can be used as an alternative to running load balancing software on Compute Engine instances. Additionally, Compute Engine utilizes standard Google data center hardware, which greatly simplifies the process of creating additional zones and regions as the service grows. This paper presents a snapshot of the functionality as of September 2013. Please sign up for the Google Cloud Platform <a href="/newsletter/">newsletter</a> to stay up to date on the rapid deployment of new features.</p>

  <h2>Sample Application</h2>
  <p>A sample application is provided to create a high availability web application running in multiple availability zones. The application contains a Python tool for automatically creating a LAMP stack with load balancers, web servers, and database servers. MySQL is utilized in a master-slave configuration and the tool can be used to promote a slave to master in preparation for a maintenance window. Once the master database is successfully promoted, all load balancers, web servers, and database servers in the availability zone can be removed by the tool.  The &ldquo;<a href="high-availability-lamp-stack-on-google-compute-engine.html">High Availability Lamp Stack on Google Compute Engine</a>&rdquo; tutorial details how to use the tool that is available on GitHub.</p>

  <h2>Conclusion</h2>
  <p>Developers now have access to a global interconnected grid of data centers on which to build applications that can achieve true high availability and are no longer constrained to a single region. Google Compute Engine is the best place for developers who require virtual machines and want to take advantage of Google&rsquo;s consistent, high-performance infrastructure. Developers are now empowered to push the boundaries of what is possible by combining new ideas with the speed, consistency, and innovation of Google Cloud Platform.</p>

</div>
<!-- /maia-main -->
</div>
</div>
</div>


<script src="//ajax.googleapis.com/ajax/libs/angularjs/1.2.15/angular.js"></script>
<script src="//ajax.googleapis.com/ajax/libs/angularjs/1.2.15/angular-sanitize.js"></script>
<script src="/js/base.min.js"></script>
<script>
  new lfl.ui.ScrollNav({});
  new lfl.cloud.GlobalProductMenu();
</script>

</body>
</html>
