{% comment %}
This document is sourced from GitHub. 
Do not change this file in a CL!
Instead, edit me at: 
https://github.com/GoogleCloudPlatformSite/GoogleCloudPlatformSite.github.io/edit/master/developers/articles/mapping-the-secrets-of-the-universe-with-google-compute-engine.html
{% endcomment %}
<!DOCTYPE html>
<html devsite>
<head>
    <title>Mapping the Secrets of the Universe with Google Compute Engine </title>
    
      <meta name="description" content="Google Cloud Platform lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.">
    
    <meta name="hide_page_heading" value="true" />
    <meta name="full_width" value="true" />
</head>
<body>

<div id="maia-main" class="cp-article">
  <div class="maia-cols">
    <div class="maia-col-9">
      <div>
        <div style="float:right">
          <div class="g-plusone"></div>
        </div>
        <h1 class="title">Mapping the Secrets of the Universe with Google Compute Engine</h1>
      </div>
      
<div class="cp-article-tutorial">

  <h2>Organization</h2>
  <p>ATLAS is a particle physics experiment at the Large Hadron Collider at CERN in Geneva, Switzerland. The ATLAS detector is searching for new discoveries in the head-on collisions of protons and heavy ions of extraordinarily high energy. <a href="http://en.wikipedia.org/wiki/ATLAS_experiment">ATLAS</a> uses a grid computing paradigm to organize its distributed resources. Worldwide, it successfully manages nearly 200 sites for more than 3,000 researchers. Recently, ATLAS used Google Compute Engine to extend its capacity and satisfy demand for steadily increasing workloads. This project successfully demonstrated the ease with which Google Compute Engine could be integrated seamlessly with the ATLAS compute grid, and that production physics research could be performed in the cloud.</p>

  <h2>Challenge</h2>
  <p>All available resources of the ATLAS Compute Grid run fully stressed year-round. According to ATLAS, user and group analysis of observational data as well as event generators and detector simulations can exceed one million jobs per day. However, they find that demand often surpasses twice that, and at various times throughout the year can range far beyond the grid&rsquo;s capacity of more than 100,000 CPU cores. Such a backlog can create scheduling and prioritization pressure on the system which can slow down research. Essentially, the lack of computational resources slows the pace of scientific discovery. So ATLAS set up an R&amp;D project to explore virtualization and validate whether or not cloud computing could be used to cope with the spikes in demand for computational resources.</p>

  <h2>Solution</h2>
  <p>ATLAS began using Google Compute Engine in the Fall of 2012. The first project was directed at the construction of high performance analysis clusters like those that are used by ATLAS for interactive and batch analysis of large data sets from particle physics experiments. &ldquo;Access to Google Compute Engine allowed us to build and test large <a href="http://root.cern.ch/drupal/content/proof">PROOF</a> clusters - up to 1,000 workers&rdquo; said Dr. Sergey Panitkin of Brookhaven National Lab, Head of Cloud Computing R&amp;D for ATLAS. These parallel data processing clusters are sensitive to the performance of the underlying storage infrastructure. Memory-only, scratch and persistent disk configurations were all benchmarked.</p>

  <p>The second effort focused on adding capacity in the form of 4,000 cores that were organized as an <a href="http://research.cs.wisc.edu/htcondor/">HTCondor</a>-based <a href="http://iopscience.iop.org/1742-6596/219/6/062028">PanDA</a> queue and transparently included into the ATLAS compute grid. This cluster was built to run CPU-intensive Monte Carlo simulation workloads. The goal was to test the long-term stability of a cloud-based cluster similar in size to that of an large ATLAS (Tier-2) grid site. An important requirement was that it be run as a full production platform, not merely as an R&amp;D proof-of-concept.</p>

  <h2>Results</h2>
  <p>ATLAS reported the scalability and performance of the Google Compute Engine infrastructure enabled them to run I/O-intensive workloads at unprecedented levels. This was the first time they were able to conduct tests at this scale. Using a 500-worker PROOF analysis cluster (running exclusively on Google Compute Engine) that was able to sustain throughput at 210 million cycles per second, the team observed opportunities for optimizations that were not evident running similar tests on infrastructure at their own facilities.</p>

  <p>Additionally, the simulation cluster ran for nearly two months as part of the ATLAS distributed compute grid, logging over 5 million core-hours, completing 458,000 computationally intensive jobs and processing about 214 million events. The cluster achieved sustained peak throughput of 15,000 jobs per day. &ldquo;We had a great experience with Google Compute Engine &hellip; and think that it is modern cloud infrastructure that can serve as a stable, high performance platform for scientific computing&rdquo; concluded Dr. Panitkin.</p>


</div>
<!-- /maia-main -->

    </div>
    <div class="maia-col-3">
      <br><br>
      
  <!-- <img src="/images/articles/mapping-the-secrets/ATLAS-chrome-logo-blue-wh_lo.png" alt="Atlas Experiment"> -->
  <p class="qte">&ldquo;The project was very stable on the cloud side - Google Compute Engine was rock solid &hellip; no failures were due to GCE problems&rdquo;.</p>
  <p class="qte-author">- Dr. Sergey Panitkin, Brookhaven National Lab</p>

    </div>
  </div>
</div>


</body>
</html>
